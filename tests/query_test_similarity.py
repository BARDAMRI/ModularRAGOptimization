import numpy as np
from llama_index.core.schema import MetadataMode

from config import INDEX_SOURCE_URL
from modules.indexer import load_vector_db
from utility.embedding_utils import get_query_vector


def embed_node_like_llamaindex(node, embedding_model):
    """
    ×ž×—×§×” ×‘×“×™×•×§ ××ª ×”×ª×”×œ×™×š ×©×œ LlamaIndex ×œ×™×¦×™×¨×ª embeddings
    """
    # 1. ×§×‘×œ ××ª ×”×˜×§×¡×˜ ×›×ž×• ×©-LlamaIndex ×¢×•×©×” - ×¢× metadata
    text_to_embed = node.get_content(metadata_mode=MetadataMode.EMBED)

    # 2. ×”×©×ª×ž×© ×‘×¤×•× ×§×¦×™×” ×”× ×›×•× ×” ×œ×§×™×“×•×“ documents (×œ× queries)
    try:
        # × ×¡×” ×¢× get_text_embedding (×”×¤×•× ×§×¦×™×” ×”×¡×˜× ×“×¨×˜×™×ª ×œ×“×•×§×•×ž× ×˜×™×)
        embedding = embedding_model.get_text_embedding(text_to_embed)
    except Exception as e:
        print(f"âš ï¸  ×©×’×™××” ×‘×§×™×“×•×“: {e}")
        # fallback - × ×¡×” ×‘×œ×™ ×¤×¨×ž×˜×¨×™× × ×•×¡×¤×™×
        embedding = embedding_model.get_text_embedding(text_to_embed)

    # 3. ×”×ž×¨ ×œ× ×ž×¤×™ array
    embedding = np.array(embedding)

    # 4. ×‘×“×•×§ ×× ×”×ž×•×“×œ ×¢×•×©×” normalization ××•×˜×•×ž×˜×™
    embedding_norm = np.linalg.norm(embedding)
    print(f"× ×•×¨×ž×” ×©×œ ×”×•×•×§×˜×•×¨ ×”×’×•×œ×ž×™: {embedding_norm:.6f}")

    # 5. normalize ×¨×§ ×× ×”×ž×•×“×œ ×œ× ×¢×•×©×” ×–××ª ×‘×¢×¦×ž×•
    if embedding_norm > 1.1 or embedding_norm < 0.9:  # ×œ× ×ž× ×•×¨×ž×œ
        embedding = normalize(embedding)
        print("âœ… ×‘×™×¦×¢×ª×™ normalization ×™×“× ×™")
    else:
        print("âœ… ×”×•×•×§×˜×•×¨ ×›×‘×¨ ×ž× ×•×¨×ž×œ")

    return embedding


def normalize(v):
    norm = np.linalg.norm(v)
    return v / norm if norm != 0 else v


def analyze_text_differences(node):
    """
    ×‘×•×“×§ ×”×‘×“×œ×™× ×‘×™×Ÿ ×”×˜×§×¡×˜ ×”×’×•×œ×ž×™ ×œ×˜×§×¡×˜ ×”×ž×¢×•×‘×“
    """
    original_text = node.text

    # ×˜×§×¡×˜ ×¢× metadata ×›×ž×• ×©-LlamaIndex ×ž×©×ª×ž×©
    embed_text = node.get_content(metadata_mode=MetadataMode.EMBED)

    print("ðŸ” × ×™×ª×•×— ×”×˜×§×¡×˜")
    print("-" * 50)
    print(f"××•×¨×š ×˜×§×¡×˜ ×ž×§×•×¨×™: {len(original_text)}")
    print(f"××•×¨×š ×˜×§×¡×˜ ×œ×§×™×“×•×“: {len(embed_text)}")

    if original_text != embed_text:
        print("âš ï¸  ×”×˜×§×¡×˜ ×”×©×ª× ×”!")
        print("×˜×§×¡×˜ ×ž×§×•×¨×™ (100 ×ª×•×•×™× ×¨××©×•× ×™×):")
        print(repr(original_text[:100]))
        print("×˜×§×¡×˜ ×œ×§×™×“×•×“ (100 ×ª×•×•×™× ×¨××©×•× ×™×):")
        print(repr(embed_text[:100]))

        # ×‘×“×•×§ metadata
        if hasattr(node, 'metadata') and node.metadata:
            print(f"Metadata ×§×™×™×: {node.metadata}")
    else:
        print("âœ… ×”×˜×§×¡×˜ ×–×”×”")

    return embed_text


# Load the vector database and embedding model
vector_db, embedding_model = load_vector_db(source="url", source_path=INDEX_SOURCE_URL)

# ×‘×“×•×§ ××ª ×¡×•×’ ×”×ž×•×“×œ
print(f"ðŸ” ×ž×™×“×¢ ×¢×œ ×”×ž×•×“×œ")
print("-" * 50)
print(f"×¡×•×’ ×”×ž×•×“×œ: {type(embedding_model)}")
print(f"×©× ×”×ž×•×“×œ: {getattr(embedding_model, 'model_name', '×œ× ×™×“×•×¢')}")

# ×¨×©×™×ž×ª methods ×–×ž×™× ×™×
methods = [method for method in dir(embedding_model) if 'embed' in method.lower()]
print(f"Methods ×–×ž×™× ×™×: {methods}")

# Access internal vector store
vector_store = getattr(vector_db, "_vector_store", None)

# Prepare a query
query = "This is a sample query."
query_vector = normalize(get_query_vector(query, embedding_model))

# Query the VectorStoreIndex
nodes = vector_db.as_retriever(similarity_top_k=1).retrieve(query)
node_with_score = nodes[0]
retrieved_node = node_with_score.node
retrieved_score = node_with_score.score
node_id = retrieved_node.node_id

# === TEXT ANALYSIS ===
processed_text = analyze_text_differences(retrieved_node)

# === EMBEDDING ANALYSIS ===
print("\nðŸ” × ×™×ª×•×— Embeddings")
print("-" * 50)

# Get stored vector from the index
stored_vector = normalize(np.array(vector_store.data.embedding_dict[node_id]))
print(f"×•×•×§×˜×•×¨ ×©×ž×•×¨ - × ×•×¨×ž×”: {np.linalg.norm(np.array(vector_store.data.embedding_dict[node_id])):.6f}")

# Re-encode using LlamaIndex's exact method
reencoded_vector = embed_node_like_llamaindex(retrieved_node, embedding_model)

# Re-encode using original text (for comparison)
original_text = retrieved_node.text
reencoded_original = normalize(np.array(embedding_model.get_text_embedding(original_text)))

# Cosine similarities
cosine_vs_stored = np.dot(query_vector, stored_vector)
cosine_vs_reencoded = np.dot(query_vector, reencoded_vector)
cosine_vs_original = np.dot(query_vector, reencoded_original)

# Distances
distance_llamaindex = np.linalg.norm(reencoded_vector - stored_vector)
distance_original = np.linalg.norm(reencoded_original - stored_vector)

print(f"\nðŸ“Š ×ª×•×¦××•×ª ×”×©×•×•××”:")
print(f"Cosine - stored vector:           {retrieved_score:.6f}")
print(f"Cosine - manual stored:           {cosine_vs_stored:.6f}")
print(f"Cosine - LlamaIndex style:        {cosine_vs_reencoded:.6f}")
print(f"Cosine - original text only:      {cosine_vs_original:.6f}")

print(f"\nðŸ“ ×ž×¨×—×§×™×:")
print(f"||LlamaIndex style - stored||:    {distance_llamaindex:.6f}")
print(f"||Original text - stored||:       {distance_original:.6f}")

# ×‘×“×™×§×ª ×”×¦×œ×—×”
if distance_llamaindex < 1e-5:
    print("âœ… ×”×¦×œ×—× ×•! ×”×•×•×§×˜×•×¨×™× ×–×”×™×")
elif distance_llamaindex < distance_original:
    print("ðŸ”¶ ×”×©×™×¤×•×¨ ×—×œ×§×™ - ×™×© ×¢×“×™×™×Ÿ ×”×‘×“×œ ×§×˜×Ÿ")
else:
    print("âŒ ×¢×“×™×™×Ÿ ×™×© ×‘×¢×™×” - ×¦×¨×™×š ×œ×—×§×•×¨ ×¢×•×“")

# ×ž×™×“×¢ × ×•×¡×£ ×œ×“×™×‘×•×’
print(f"\n×ž×™×ž×“×™ ×•×•×§×˜×•×¨×™×:")
print(f"Stored: {stored_vector.shape}")
print(f"Re-encoded: {reencoded_vector.shape}")
print(f"×“×•×’×ž×ª ×¢×¨×›×™× (5 ×¨××©×•× ×™×):")
print(f"Stored:     {stored_vector[:5]}")
print(f"Re-encoded: {reencoded_vector[:5]}")
