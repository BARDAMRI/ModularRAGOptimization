"""
User interface utilities for selecting vector database configurations
"""
import os
import sys
import termios
from datetime import datetime
from enum import Enum
from typing import Optional, Tuple, Callable

import torch
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from sentence_transformers import CrossEncoder
from transformers import AutoTokenizer

from configurations.config import NQ_SAMPLE_SIZE, MAX_NEW_TOKENS, TEMPERATURE, INDEX_SOURCE_URL, RETRIEVER_TOP_K, \
    ACTIVE_QA_DATASET, EVALUATOR_TYPE
from configurations.config import TRILATERATION_ITERATIVE, TRILATERATION_MAX_REFINES, TRILATERATION_CONVERGENCE_TOL
from experiments.noise_experiment import run_noise_experiment
from experiments.retrieval_experiment import run_retrieval_base_experiment
from matrics.results_logger import ResultsLogger
from modules.model_loader import load_model, load_evaluator_model
from modules.query import process_query_with_context
from scripts.evaluator import enumerate_top_documents
from scripts.qa_data_set_loader import load_qa_queries
from utility.device_utils import get_optimal_device
from utility.logger import logger
from vector_db.indexer import get_embedding_model_info, load_vector_db
from vector_db.storing_methods import StoringMethod
from vector_db.trilateration_retriever import cosine_distance
from vector_db.vector_db_interface import VectorDBInterface

PROJECT_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


def select_dataset_and_corpus():
    from configurations.config import QA_DATASETS
    print_section_header("ðŸŽ“ DATASET SELECTION")

    # Add manual mode at the top
    print("0. âŒ No Dataset (manual queries only)")
    dataset_keys = list(QA_DATASETS.keys())
    for i, key in enumerate(dataset_keys, 1):
        entry = QA_DATASETS[key]
        desc = entry.get("description", "")
        corpus = entry.get("corpus_source", "âŒ None defined")
        print(f"{i}. {key} - {desc}\n   â†³ Corpus: {corpus}")

    options = [(str(i + 1), k) for i, k in enumerate(dataset_keys)]
    from configurations.config import ACTIVE_QA_DATASET
    default_label = f"(default: {ACTIVE_QA_DATASET})" if ACTIVE_QA_DATASET else "(default: No Dataset)"
    selected = ask_selection(
        prompt_text=f"\nSelect dataset (0-{len(dataset_keys)}) {default_label}: ",
        options=options,
        default=None
    )
    if not selected or selected.strip() == "":
        if ACTIVE_QA_DATASET:
            print(f"\nâœ… Using default dataset from configuration: {ACTIVE_QA_DATASET}")
            dataset_key = ACTIVE_QA_DATASET
            dataset_cfg = QA_DATASETS.get(dataset_key, {})
            corpus_source = dataset_cfg.get("corpus_source")
            if corpus_source:
                print(f"ðŸ“š Using predefined corpus: {corpus_source}")
            else:
                print("âš ï¸ No corpus defined for this dataset. You will need to configure a Vector DB manually.")
            return dataset_key, corpus_source
        else:
            print("\nâš ï¸ No default dataset configured. Falling back to manual mode.")
            return None, None

    if selected == "manual" or selected == "0":
        print("\nâš ï¸ Manual mode selected â€” no predefined dataset or corpus.")
        print("You can still use a custom vector database or run without one.")
        return None, None

    # Otherwise, selected is a dataset key
    # If selected is a number, map to dataset_keys index
    try:
        idx = int(selected) - 1
        dataset_key = dataset_keys[idx]
    except (ValueError, IndexError):
        dataset_key = selected
    dataset_cfg = QA_DATASETS[dataset_key]
    corpus_source = dataset_cfg.get("corpus_source")

    print(f"\nâœ… Selected dataset: {dataset_key}")
    if corpus_source:
        print(f"ðŸ“š Using predefined corpus: {corpus_source}")
    else:
        print("âš ï¸ No corpus defined for this dataset. You will need to configure a Vector DB manually.")

    return dataset_key, corpus_source


############################################################
# Special Command Handling (command-line and interactive)   #
############################################################

def print_help():
    print("\nðŸ“– Available Special Commands:")
    print("  ðŸ†˜ --help           - Show this help message")
    print("  ðŸ§¹ --clear-cache    - Clear all caches")
    print("  ðŸ“ˆ --performance    - Show performance report")
    print("  ðŸ’¾ --cache-stats    - Show cache statistics")
    print("  ðŸ§‘â€ðŸ”¬ --analyze       - Run analysis mode")
    print("  ðŸšª exit/quit/q      - Exit or return to main menu (where applicable)")
    print()


def handle_command(command: str) -> bool:
    """
    Handle special commands interactively or from arguments.
    Returns True if a special command was handled.
    """
    cmd = command.strip().lower()
    if cmd in ("--help", "help"):
        print_help()
        return True
    if cmd == "--clear-cache":
        if CACHE_AVAILABLE:
            clear_all_caches()
            print("ðŸ§¹ All caches cleared.")
        else:
            print("âš ï¸  Cache clearing not available.")
        return True
    if cmd == "--performance":
        if PERFORMANCE_AVAILABLE:
            performance_report()
        else:
            print("âš ï¸  Performance monitoring not available.")
        return True
    if cmd == "--cache-stats":
        if CACHE_AVAILABLE:
            cache_stats()
        else:
            print("âš ï¸  Cache statistics not available.")
        return True
    if cmd == "--analyze":
        print("ðŸ§‘â€ðŸ”¬ Entering analysis mode...")
        # Defer to analysis mode in main program, just signal handled.
        # Actual mode switch logic should be in the main loop.
        return True
    return False


def _wrap_special_commands(fn):
    """
    Decorator to wrap input prompts to handle special commands.
    If a special command is entered, handle it, then re-prompt.
    """

    def wrapper(*args, **kwargs):
        while True:
            result = fn(*args, **kwargs)
            # Only handle str results
            if isinstance(result, str) and handle_command(result):
                continue
            return result

    return wrapper


class Mode(str, Enum):
    INTERACTIVE = "1"
    EVALUATION = "2"
    DEVELOPMENT = "3"
    ANALYSIS = "4"
    INFO = "5"
    DOWNLOAD = "6"
    EXIT = "7"


# Import performance monitoring with fallbacks
try:
    from utility.performance import (
        monitor_performance, performance_report, performance_monitor, track_performance
    )
    from utility.cache import cache_stats, clear_all_caches

    PERFORMANCE_AVAILABLE = True
    CACHE_AVAILABLE = True
except ImportError:
    logger.warning("Performance monitoring or caching not available")
    PERFORMANCE_AVAILABLE = False
    CACHE_AVAILABLE = False


    def monitor_performance(name):
        from contextlib import contextmanager
        @contextmanager
        def dummy_context():
            yield

        return dummy_context()


    def track_performance(name=None):
        def decorator(func):
            return func

        return decorator


    def performance_report():
        print("Performance monitoring not available")


    def cache_stats():
        print("Cache monitoring not available")


    def clear_all_caches():
        print("Cache clearing not available")


def prompt_with_validation(
        prompt_text: str,
        validation_fn: Callable[[str], bool],
        default: Optional[str] = None,
        transform_fn: Optional[Callable[[str], str]] = None,
        error_msg: str = "âŒ Invalid input."
) -> str:
    while True:
        user_input = input(prompt_text).strip()

        # Allow empty input to propagate upward only if no default is provided
        if not user_input:
            if default is None:
                # Let higher-level logic handle empty input
                return ""
            else:
                print(f"âœ… Using default: {default}")
                return default

        # Remove surrounding quotes if present
        if user_input.startswith('"') and user_input.endswith('"'):
            user_input = user_input[1:-1].strip()
        elif user_input.startswith("'") and user_input.endswith("'"):
            user_input = user_input[1:-1].strip()

        try:
            normalized_input = user_input.lower()
            if validation_fn(normalized_input):
                result = transform_fn(normalized_input) if transform_fn else normalized_input
                print(f"âœ… Selected: {result}")
                return result
        except Exception:
            pass

        print(error_msg)


@_wrap_special_commands
def ask_selection(prompt_text: str, options: list, default: Optional[str] = None) -> str:
    """
    Prompt the user to select one of the provided options by name or index (1-based).

    Args:
        prompt_text (str): The prompt to display.
        options (list): List of string options or (value, label) pairs.
        default (Optional[str]): Default value to use if user presses Enter.

    Returns:
        str: The selected option's value.
    """
    if all(isinstance(opt, tuple) for opt in options):
        value_label_pairs = options
    else:
        value_label_pairs = [(opt.lower(), opt) for opt in options]

    valid_values = [str(v) for v, _ in value_label_pairs]
    index_map = {str(i + 1): v for i, (v, _) in enumerate(value_label_pairs)}

    def validate(user_input: str) -> bool:
        return user_input.lower() in valid_values or user_input in index_map

    def transform(user_input: str) -> str:
        return index_map[user_input] if user_input in index_map else user_input.lower()

    return prompt_with_validation(
        prompt_text=prompt_text,
        validation_fn=validate,
        transform_fn=transform,
        default=default,
        error_msg=f"âŒ Please choose a number (1-{len(value_label_pairs)}) or one of: {', '.join(valid_values)}")


def retry_or_exit(message: str, retry_prompt: str = "Retry? (y/n): ") -> bool:
    print(f"âŒ {message}")
    logger.error(message)
    return ask_yes_no(retry_prompt, default="n")


@_wrap_special_commands
def ask_yes_no(prompt_text: str, default: str = "y") -> bool:
    result = prompt_with_validation(
        prompt_text=prompt_text,
        validation_fn=lambda x: x.lower() in ['y', 'yes', 'n', 'no'],
        transform_fn=lambda x: x.lower(),
        default=default,
        error_msg="âŒ Please enter 'y' or 'n'."
    )
    return result in ['y', 'yes']


def print_section_header(title: str):
    """
    Print a section header with a consistent visual style.

    Args:
        title (str): The title to display in the header.
    """
    print("\n" + "=" * 60)
    print(f"{title}")
    print("=" * 60)


@_wrap_special_commands
def ask_nonempty_string(prompt_text: str, default: Optional[str] = None) -> str:
    """
    Prompt user for non-empty string input.

    Args:
        prompt_text (str): The prompt message to display.
        default (Optional[str]): Optional default value if user presses Enter.

    Returns:
        str: Validated non-empty input from the user.
    """
    return prompt_with_validation(
        prompt_text=prompt_text,
        validation_fn=lambda x: bool(x.strip()),
        default=default,
        error_msg="âŒ Please enter a valid value."
    )


# Command-line argument handler for special commands
def handle_command_line_args(args=None):
    """
    Scan sys.argv or given args for special commands and execute them.
    Returns True if execution should stop before main program.
    """
    import sys
    argv = args if args is not None else sys.argv[1:]
    handled_any = False
    for arg in argv:
        if arg.startswith("--") and handle_command(arg):
            handled_any = True
    return handled_any


def display_storing_methods():
    print_section_header("ðŸ“Š AVAILABLE VECTOR DATABASE STORING METHODS")

    descriptions = StoringMethod.get_descriptions()
    recommendations = StoringMethod.get_recommendations()

    for i, method in enumerate(StoringMethod.get_all_methods(), 1):
        print(f"\n{i}. {method.upper()}")
        print(f"   Description: {descriptions[method]}")
        print(f"   {recommendations[method]}")

    print("\n" + "=" * 60)


def get_user_storing_method(default_method: Optional[str] = None) -> str:
    display_storing_methods()
    methods = StoringMethod.get_all_methods()
    return ask_selection(
        prompt_text=f"\nSelect a storing method (1-{len(methods)}) or press Enter for default ({default_method}): ",
        options=methods,
        default=default_method
    )


import re
from urllib.parse import urlparse


def detect_source_type(source: str) -> str:
    """Detect the type of the input source."""
    if re.match(r"^\w+:\w+$", source):  # HuggingFace dataset with config
        return "huggingface_dataset_with_config"
    elif re.match(r"^\w+$", source):  # HuggingFace dataset without config
        return "huggingface_dataset"
    elif re.match(r"^https?://", source):  # URL
        parsed = urlparse(source)
        if parsed.netloc and parsed.path:
            return "url"
    elif os.path.exists(source):  # Local path
        return "local_path"
    return "invalid"


def get_user_source_path() -> str:
    print_section_header("ðŸ“ DATA SOURCE CONFIGURATION")
    print("Enter the path to your data source. Supported formats:")
    print("  â€¢ Local directory: /path/to/your/documents")
    print("  â€¢ URL: https://example.com/data.txt")
    print("  â€¢ HuggingFace dataset: squad:plain_text")
    print("  â€¢ HuggingFace dataset (no config): wikitext")
    print(f"\nðŸ” Press Enter to use default as configured in config.INDEX_SOURCE_URL: {INDEX_SOURCE_URL}")

    return prompt_with_validation(
        prompt_text="ðŸ”¤ Source path: ",
        validation_fn=lambda x: detect_source_type(x) != "invalid",
        default=INDEX_SOURCE_URL,
        transform_fn=lambda x: x.strip().strip('"').strip("'"),
        error_msg="âŒ Invalid path format. Please try again."
    )


def confirm_configuration(storing_method: str, source_path: str, distance_function: Optional[str] = None) -> bool:
    print_section_header("âš™ï¸  CONFIGURATION SUMMARY")
    print(f"Storing Method: {storing_method}")
    print(f"Source Path:    {source_path}")

    descriptions = StoringMethod.get_descriptions()
    if storing_method in descriptions:
        print(f"Description:    {descriptions[storing_method]}")

    if storing_method.lower() == "chroma" and distance_function:
        print(f"Distance Metric: {distance_function}")

    print("=" * 60)
    return ask_yes_no("Proceed with this configuration? (y/n): ", default="y")


def interactive_vector_db_setup() -> Tuple[str, str, Optional[str]]:
    """
    Complete interactive setup for vector database configuration.

    Returns:
        Tuple[str, str, Optional[str]]: (storing_method, source_path, distance_function)
    """
    print("\nðŸ¤– VECTOR DATABASE SETUP WIZARD")
    print("Welcome! Let's configure your vector database.")

    try:
        storing_method = get_user_storing_method()
        source_path = get_user_source_path()
        distance_function = get_user_distance_function(storing_method)

        if confirm_configuration(storing_method, source_path, distance_function):
            logger.info(
                f"User selected configuration: method={storing_method}, source={source_path}, distance={distance_function}")
            return storing_method, source_path, distance_function
        else:
            print("\nðŸ”„ Let's try again...")
            return interactive_vector_db_setup()

    except KeyboardInterrupt:
        print("\n\nâŒ Setup cancelled by user.")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ An error occurred during setup: {e}")
        logger.error(f"Error in interactive setup: {e}")
        sys.exit(1)


def quick_setup(method: Optional[str] = None, source: Optional[str] = None) -> Tuple[str, str]:
    """
    Quick setup with minimal prompts. Falls back to interactive if parameters missing.

    Args:
        method (Optional[str]): Storing method (if None, will prompt)
        source (Optional[str]): Source path (if None, will prompt)

    Returns:
        Tuple[str, str]: (storing_method, source_path)
    """
    # Validate and get method
    if method and StoringMethod.is_valid_method(method):
        storing_method = method
        print(f"âœ… Using method: {storing_method}")
    else:
        if method:
            print(f"âš ï¸  Invalid method '{method}', please select a valid one:")
        storing_method = get_user_storing_method()

    # Get source path
    if source:
        source_path = source
        print(f"âœ… Using source: {source_path}")
    else:
        source_path = get_user_source_path()

    return storing_method, source_path


# Convenience function for command line usage
def setup_from_args(args) -> Tuple[str, str]:
    """
    Setup configuration from command line arguments.

    Args:
        args: Argument parser object with 'method' and 'source' attributes

    Returns:
        Tuple[str, str]: (storing_method, source_path)
    """
    method = getattr(args, 'method', None)
    source = getattr(args, 'source', None)

    if method and source:
        # Both provided, validate and use
        if not StoringMethod.is_valid_method(method):
            retry_or_exit(f"Invalid storing method: {method}")

        print(f"âœ… Using command line configuration:")
        print(f"   Method: {method}")
        print(f"   Source: {source}")
        return method, source
    else:
        # Missing parameters, use interactive setup
        print("âš ï¸  Missing configuration parameters, starting interactive setup...")
        return interactive_vector_db_setup()


def startup_initialization() -> Tuple[AutoTokenizer, torch.nn.Module, CrossEncoder]:
    """Initialize model and display startup information."""

    print("ðŸš€ Loading model with MPS support...")
    with monitor_performance("startup_model_loading"):
        tokenizer, model = load_model()
        cross_encoder_model = load_evaluator_model()
        print(f"âœ… Model loaded on device: {next(model.parameters()).device}")
        return tokenizer, model, cross_encoder_model


def display_startup_banner():
    """Display the application startup banner."""
    print("\n" + "=" * 60)
    print("ðŸ¤– RAG SYSTEM - Enhanced MPS Support")
    print("=" * 60)
    device = get_optimal_device()
    if device.type == "mps":
        print("ðŸŽ Apple Silicon GPU Detected - MPS Enabled")
    elif device.type == "cuda":
        print("ðŸ”¥ NVIDIA GPU Detected - CUDA Enabled")
    else:
        print("ðŸ’» CPU Mode - No GPU Acceleration")
    print("=" * 60)
    return device


def get_user_distance_function(storing_method: str) -> Optional[str]:
    if storing_method.lower() != "chroma":
        return None

    print("\nðŸ“ Select distance function for similarity search:")
    print("1. Cosine")
    print("2. Euclidean (L2)")
    print("3. Inner Product")

    choice = input("Enter your choice [1-3]: ").strip()

    mapping = {
        "1": "cosine",
        "2": "l2",
        "3": "ip"
    }

    return mapping.get(choice, "cosine")  # fallback to cosine


from utility.distance_metrics import DistanceMetric


def setup_vector_database():
    """Setup vector database with user interaction and dataset/corpus selection."""

    print("\nðŸ“š VECTOR DATABASE SETUP")
    print("-" * 30)

    # Step 1: Dataset selection and corpus association
    dataset_key, corpus_source = select_dataset_and_corpus()

    # If user chose manual mode (no dataset), skip corpus and go to vector DB selection
    if dataset_key is None:
        print("\nâš ï¸ No dataset selected. Skipping dataset/corpus loading.")
        # Fallback to interactive or no-DB mode
        print("(Default: y - use Vector DB)")
        use_vector_db = ask_yes_no("Do you want to use vector database for context retrieval? (y/n): ", default='y')
        if not use_vector_db:
            print("ðŸ“ Running in simple Q&A mode (no context retrieval)")
            return None, None, None, None, None
        try:
            storing_method, source_path, distance_function = interactive_vector_db_setup()
            if distance_function:
                try:
                    distance_metric = DistanceMetric(distance_function.lower())
                except ValueError:
                    print(f"âš ï¸ Unsupported distance metric '{distance_function}', falling back to COSINE")
                    distance_metric = DistanceMetric.COSINE
            else:
                distance_metric = DistanceMetric.COSINE
            with monitor_performance("vector_db_loading"):
                logger.info(
                    f"Loading Vector DB with method: {storing_method}, source: {source_path}, distance: {distance_metric}")
                vector_db, embedding_model = load_vector_db(
                    source_path=source_path,
                    storing_method=storing_method,
                    distance_metric=distance_metric
                )
                print("âœ… Vector DB loaded successfully")
                stats = vector_db.get_stats()
                print(f"\nðŸ“Š Database Statistics:")
                for key, value in stats.items():
                    print(f"  â€¢ {key}: {value}")
                from configurations.config import HF_MODEL_NAME
                print("HF_MODEL_NAME:", HF_MODEL_NAME)
                emb = embedding_model.get_text_embedding("test")
                print("Embedding dimension:", len(emb))
                print("Vector DB stats:", vector_db.get_stats())
                return vector_db, embedding_model, storing_method, source_path, distance_metric
        except Exception as e:
            logger.error(f"Failed to load vector DB: {e}")
            print(f"âŒ Vector DB setup failed: {e}")
            print("(Default: y - retry with Vector DB)")
            retry = ask_yes_no("Retry with Vector DB? (y/n): ", default='y')
            if not retry:
                print("âš ï¸  Continuing without vector database...")
                return None, None, None, None, None
            else:
                # Retry the same setup function to re-download/rebuild
                return setup_vector_database()

    # If the dataset defines a corpus, skip vector DB prompts and use it
    if corpus_source:
        # Use default storing method and distance metric if desired, or prompt if needed
        from configurations.config import DEFAULT_STORING_METHOD, DEFAULT_DISTANCE_METRIC
        if DEFAULT_STORING_METHOD:
            storing_method = DEFAULT_STORING_METHOD
            print(f"âœ… Using storing method from config: {storing_method}")
        else:
            print("âš™ï¸ No storing method defined in config. Using default: chroma")
            storing_method = "chroma"

        if DEFAULT_DISTANCE_METRIC:
            try:
                distance_metric = DistanceMetric(DEFAULT_DISTANCE_METRIC.lower())
                print(f"âœ… Using distance metric from config: {DEFAULT_DISTANCE_METRIC}")
            except ValueError:
                print(f"âš ï¸ Invalid distance metric '{DEFAULT_DISTANCE_METRIC}' in config. Falling back to COSINE.")
                distance_metric = DistanceMetric.COSINE
        else:
            print("âš™ï¸ No distance metric defined in config. Using default: COSINE")
            distance_metric = DistanceMetric.COSINE
        source_path = corpus_source
        print(f"\nðŸ”„ Automatically loading vector DB for dataset '{dataset_key}' using corpus '{corpus_source}'...")
        try:
            with monitor_performance("vector_db_loading"):
                from utility.embedding_utils import get_text_embedding
                logger.info(
                    f"Loading Vector DB with method: {storing_method}, source: {source_path}, distance: {distance_metric}")
                vector_db, embedding_model = load_vector_db(
                    source_path=source_path,
                    storing_method=storing_method,
                    distance_metric=distance_metric
                )
                print("âœ… Vector DB loaded successfully")
                stats = vector_db.get_stats()
                print(f"\nðŸ“Š Database Statistics:")
                for key, value in stats.items():
                    print(f"  â€¢ {key}: {value}")
                from configurations.config import HF_MODEL_NAME
                print("HF_MODEL_NAME:", HF_MODEL_NAME)
                emb = embedding_model.get_text_embedding("test")
                print("Embedding dimension:", len(emb))
                print("Vector DB stats:", vector_db.get_stats())
                return vector_db, embedding_model, storing_method, source_path, distance_metric
        except Exception as e:
            logger.error(f"Failed to load vector DB: {e}")
            print(f"âŒ Vector DB setup failed: {e}")
            if "No files found" in str(e) and corpus_source:
                print(f"ðŸŒ Corpus '{corpus_source}' not found locally. Attempting to rebuild via load_vector_db...")
                try:
                    vector_db, embedding_model = load_vector_db(
                        source_path=corpus_source,
                        storing_method=storing_method,
                        distance_metric=distance_metric
                    )
                    print("âœ… Vector DB successfully built or downloaded by load_vector_db().")
                    stats = vector_db.get_stats()
                    print(f"ðŸ“Š Database Statistics:")
                    for key, value in stats.items():
                        print(f"  â€¢ {key}: {value}")
                    return vector_db, embedding_model, storing_method, source_path, distance_metric
                except Exception as rebuild_e:
                    print(f"âŒ Rebuild via load_vector_db() failed for corpus '{corpus_source}': {rebuild_e}")
                    print("âš ï¸  Continuing without vector database...")
                    return None, None, None, None, None
            else:
                print("(Default: y - retry with Vector DB)")
                retry = ask_yes_no("Retry with Vector DB? (y/n): ", default='y')
                if not retry:
                    print("âš ï¸  Continuing without vector database...")
                    return None, None, None, None, None
                else:
                    # Retry the same setup function to re-download/rebuild
                    return setup_vector_database()
    else:
        # No corpus defined: fall back to legacy interactive vector DB setup
        print("(Default: y - use Vector DB)")
        use_vector_db = ask_yes_no("Do you want to use vector database for context retrieval? (y/n): ", default='y')
        if not use_vector_db:
            print("ðŸ“ Running in simple Q&A mode (no context retrieval)")
            return None, None, None, None, None
        try:
            storing_method, source_path, distance_function = interactive_vector_db_setup()
            if distance_function:
                try:
                    distance_metric = DistanceMetric(distance_function.lower())
                except ValueError:
                    print(f"âš ï¸ Unsupported distance metric '{distance_function}', falling back to COSINE")
                    distance_metric = DistanceMetric.COSINE
            else:
                distance_metric = DistanceMetric.COSINE
            with monitor_performance("vector_db_loading"):
                logger.info(
                    f"Loading Vector DB with method: {storing_method}, source: {source_path}, distance: {distance_metric}")
                vector_db, embedding_model = load_vector_db(
                    source_path=source_path,
                    storing_method=storing_method,
                    distance_metric=distance_metric
                )
                print("âœ… Vector DB loaded successfully")
                stats = vector_db.get_stats()
                print(f"\nðŸ“Š Database Statistics:")
                for key, value in stats.items():
                    print(f"  â€¢ {key}: {value}")
                from configurations.config import HF_MODEL_NAME
                print("HF_MODEL_NAME:", HF_MODEL_NAME)
                emb = embedding_model.get_text_embedding("test")
                print("Embedding dimension:", len(emb))
                print("Vector DB stats:", vector_db.get_stats())
                return vector_db, embedding_model, storing_method, source_path, distance_metric
        except Exception as e:
            logger.error(f"Failed to load vector DB: {e}")
            print(f"âŒ Vector DB setup failed: {e}")
            print("(Default: y - retry with Vector DB)")
            retry = ask_yes_no("Retry with Vector DB? (y/n): ", default='y')
            if not retry:
                print("âš ï¸  Continuing without vector database...")
                return None, None, None, None, None
            else:
                # Retry the same setup function to re-download/rebuild
                return setup_vector_database()


def validate_prerequisites(mode: str, dataset_key, vector_db) -> bool:
    """
    Validate that the required components are available before running a mode.
    """
    if mode == "evaluation" and (dataset_key is None or vector_db is None):
        print("âŒ Evaluation mode requires both a dataset and a vector database.")
        return False

    if mode == "retrieval_base" and (dataset_key is None or vector_db is None):
        print("âŒ Retrieval Base experiment requires both a dataset and a vector database.")
        return False

    if mode == "noise" and vector_db is None:
        print("âŒ Noise robustness test requires a vector database.")
        return False

    if mode == "development" and vector_db is None:
        print("âš ï¸ Development mode may have limited functionality without a vector DB.")

    return True


def display_main_menu(vector_db=None, dataset_key=None):
    """Display the main menu and get user choice."""
    print_section_header("ðŸŽ¯ SELECT MODE")
    eval_label = "ðŸ“Š Evaluation Mode" if dataset_key and vector_db else "ðŸ“Š Evaluation Mode (disabled â€“ no dataset/DB)"
    dev_label = "ðŸ”§ Development Test Mode" if vector_db else "ðŸ”§ Development Test Mode (limited â€“ no DB)"

    options = [
        ("1", "ðŸ’¬ Interactive Q&A Mode"),
        ("2", eval_label),
        ("3", dev_label),
        ("4", "ðŸ“ˆ Results Analysis"),
        ("5", "âš™ï¸ System Information"),
        ("6", "ðŸ“¥ï¸ Download QA Dataset"),
        ("7", "ðŸ§ª Experiments & Evaluation"),
        ("8", "ðŸšª Exit"),
        ("9", "ðŸ”„ Reset Vector DB & Embedding Model")
    ]

    for key, label in options:
        print(f"{key}. {label}")

    return ask_selection(
        prompt_text="\nEnter your choice: ",
        options=[(key, key) for key, _ in options]
    )


def show_experiments_menu():
    """
    Display the experiments and evaluation menu and get user choice.
    """
    print_section_header("ðŸ§ª EXPERIMENTS & EVALUATION")
    options = [
        ("1", "ðŸ§ª Simple Retrieval Evaluation"),
        ("2", "ðŸ”¬ Noise Robustness Experiment"),
        ("3", "ðŸ”§ Development Test Mode"),
        ("4", "ðŸ§ª Compare LLM scoring to vector DB distance"),
        ("5", "â¬…ï¸ Back to Main Menu")
    ]
    for key, label in options:
        print(f"{key}. {label}")
    return ask_selection(
        prompt_text="\nSelect experiment: ",
        options=[(key, key) for key, _ in options]
    )


def flush_input():
    """Flush accidental keyboard input from the terminal buffer."""
    try:
        termios.tcflush(sys.stdin, termios.TCIFLUSH)
    except Exception:
        pass  # Normal in some environments


def show_system_info(vector_db, model, device):
    """Display comprehensive system information."""
    print("\nâš™ï¸  SYSTEM INFORMATION")
    print("=" * 25)

    # Device information
    profile_gpu(model, device)

    # Model information
    print(f"\nðŸ¤– Model Information:")
    try:
        info = get_embedding_model_info()
        for key, value in info.items():
            print(f"  â€¢ {key}: {value}")
    except Exception as e:
        print(f"  âŒ Could not get model info: {e}")

    # Vector DB information
    if vector_db:
        print(f"\nðŸ“š Vector Database:")
        stats = vector_db.get_stats()
        for key, value in stats.items():
            print(f"  â€¢ {key}: {value}")
    else:
        print(f"\nðŸ“š Vector Database: Not loaded")

    # Performance stats
    if PERFORMANCE_AVAILABLE:
        print(f"\nðŸ“Š Performance Statistics:")
        performance_report()

    # Cache stats
    if CACHE_AVAILABLE:
        print(f"\nðŸ’¾ Cache Statistics:")
        cache_stats()


@track_performance("gpu_profiling")
def profile_gpu(model, device):
    """Enhanced GPU profiling with MPS support."""

    print(f"\nðŸ–¥ï¸  DEVICE INFORMATION")
    print("-" * 25)

    if device.type == "mps":
        print("ðŸŽ MPS (Apple Silicon) GPU detected:")
        print("  â€¢ Memory monitoring is limited on MPS")
        print("  â€¢ Using float32 precision for compatibility")
        try:
            print(f"  â€¢ Model device: {next(model.parameters()).device}")
            print(f"  â€¢ Model dtype: {next(model.parameters()).dtype}")

            bfloat16_count = sum(1 for p in model.parameters() if p.dtype == torch.bfloat16)
            if bfloat16_count > 0:
                print(f"  âš ï¸  Warning: {bfloat16_count} bfloat16 parameters detected!")
            else:
                print("  âœ… All parameters are MPS-compatible")

        except Exception as e:
            print(f"  â€¢ Could not get model info: {e}")

    elif device.type == "cuda":
        print("ðŸ”¥ CUDA GPU Utilization:")
        print(torch.cuda.memory_summary(device="cuda"))
    else:
        print("ðŸ’» CPU Mode - No GPU acceleration")


@track_performance("interactive_mode")
def run_interactive_mode(vector_db, embedding_model, tokenizer, model, device):
    """Run the interactive Q&A mode.
    This mode allows users to ask questions and get answers win a loop with optional context retrieval.

    Args:
        ""vector_db (Optional[object]): Vector database instance for context retrieval.
        embedding_model (Optional[object]): Embedding model for vectorization.
        tokenizer (Optional[object]): Tokenizer for the model.
        model (Optional[object]): Language model for generating answers.
        device (torch.device): Device to run the model on (CPU/GPU/MPS).
        Returns:
            "None

    Parameters
    ----------
    device
    model
    tokenizer
    embedding_model
    vector_db
    """

    print("\nðŸ’¬ INTERACTIVE Q&A MODE")
    print("=" * 30)

    if vector_db is None:
        print("ðŸ“ Running in simple Q&A mode (no context retrieval)")
    else:
        print("ðŸ“š Running with context retrieval enabled")
        print(f"ðŸ—ƒï¸  Database type: {vector_db.db_type}")

    print("ðŸ’¡ Type 'exit', 'quit', or 'q' to return to main menu")
    print("ðŸ’¡ Type 'help' for available commands\n")

    while True:
        flush_input()
        user_prompt = input("ðŸ¤” Your question: ").strip()

        if user_prompt.lower() in ['exit', 'quit', 'q']:
            print("ðŸ”™ Returning to main menu...")
            break

        if user_prompt.lower() == 'help':
            print("\nðŸ“– Available Commands:")
            print("  â€¢ exit/quit/q - Return to main menu")
            print("  â€¢ help - Show this help")
            print("  â€¢ stats - Show database statistics")
            print("  â€¢ device - Show device information")
            continue

        if user_prompt.lower() == 'stats' and vector_db:
            stats = vector_db.get_stats()
            print("\nðŸ“Š Database Statistics:")
            for key, value in stats.items():
                print(f"  â€¢ {key}: {value}")
            continue

        if user_prompt.lower() == 'device':
            profile_gpu(model, device)
            continue

        if not user_prompt:
            print("Please enter a question.")
            continue

        print("ðŸ§  Thinking...")
        with monitor_performance("interactive_query"):
            try:
                result = process_query_with_context(
                    user_prompt, model, tokenizer, device,
                    vector_db, embedding_model,
                    max_retries=3,
                    quality_threshold=0.5
                )

                if result["error"]:
                    logger.error(f"Error: {result['error']}")
                    print(f"âŒ Sorry, there was an error: {result['error']}")
                else:
                    logger.info(f"Question: {result['question']}, Answer: {result['answer'].strip()}")
                    print(f"\nðŸ¤– Answer: {result['answer'].strip()}")
                    if 'score' in result:
                        print(f"ðŸ“Š Confidence: {result['score']:.2f}")
                    print(f"ðŸ–¥ï¸  Device: {result.get('device_used', 'unknown')}")
                    print("-" * 50)

            except Exception as e:
                logger.error(f"Unexpected error during query processing: {e}")
                print(f"âŒ Unexpected error: {e}")


@track_performance("evaluation_mode")
def run_evaluation_mode(vector_db, embedding_model, tokenizer, model, device):
    """
    Run the evaluation mode using a QA dataset and evaluate the model's performance.

    Args:
        vector_db: Vector DB instance for context retrieval.
        embedding_model: Embedding model for vectorization.
        tokenizer: Tokenizer for the LLM.
        model: The LLM model used for answering.
        device: Device on which model is running.
    """
    if not validate_prerequisites("evaluation", ACTIVE_QA_DATASET, vector_db):
        return

    print("\nðŸ“Š EVALUATION MODE")
    print("=" * 25)

    mode_choice = ask_selection(
        prompt_text="Select mode: \n- (e)numeration \n- (t)rilateration retriever  \n- (h)ill climbing: ",
        options=["e", "t", "h"],
        default="e"
    )

    results_logger = ResultsLogger(top_k=RETRIEVER_TOP_K, mode="hill" if mode_choice == "h" else "enum")

    try:
        queries = load_qa_queries(NQ_SAMPLE_SIZE)
    except Exception as e:
        print(f"âŒ Failed to load QA dataset: {e}")
        return

    print(f"ðŸ“ Processing {len(queries)} queries from dataset '{ACTIVE_QA_DATASET}'...")
    with monitor_performance("evaluation_mode_execution"):
        for i, query in enumerate(queries):
            print(f"ðŸ” Processing query {i + 1}/{len(queries)}: {query[:80]}...")

            with monitor_performance(f"query_processing_{i + 1}"):
                try:
                    if mode_choice == "h":
                        from scripts.evaluator import hill_climb_documents
                        result = hill_climb_documents(
                            i=i, num=len(queries), query=query, index=vector_db,
                            llm_model=model, tokenizer=tokenizer,
                            embedding_model=embedding_model, top_k=RETRIEVER_TOP_K,
                            max_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE
                        )
                    elif mode_choice == "t":
                        from vector_db.trilateration_retriever import TrilaterationRetriever
                        retriever = TrilaterationRetriever(
                            embedding_model,
                            vector_db,
                            iterative=TRILATERATION_ITERATIVE,
                            max_refine_steps=TRILATERATION_MAX_REFINES,
                            convergence_tol=TRILATERATION_CONVERGENCE_TOL
                        )
                        result = retriever.retrieve(query)
                    else:
                        result = enumerate_top_documents(
                            i=i, num=len(queries), query=query, index=vector_db,
                            embedding_model=embedding_model,
                            top_k=RETRIEVER_TOP_K, convert_to_vector=False
                        )
                    log_message = (f"âœ… Query {i + 1} processed. Answer: {result.get('query', 'N/A')[:100]}, "
                                   f"with distance of : {result.get('distance', 'N/A')}...")
                    results_logger.log(log_message)

                except Exception as e:
                    logger.error(f"âŒ Error in evaluation query {i + 1}: {e}")
                    print(f"âŒ Error processing query {i + 1}: {e}")

    print("âœ… Evaluation completed.")


@track_performance("development_test")
def run_development_test(vector_db, embedding_model, tokenizer, model, device):
    """Enhanced development test with MPS compatibility."""
    validate_prerequisites("development", ACTIVE_QA_DATASET, vector_db)
    print("\nðŸ”§ DEVELOPMENT TEST MODE")
    print("=" * 30)

    # System info
    profile_gpu(model, device)

    # Test MPS compatibility
    print("\nðŸ§ª Testing MPS compatibility...")
    try:
        from modules.model_loader import test_mps_compatibility
        mps_test_result = test_mps_compatibility()
        print(f"MPS Test: {'âœ… PASSED' if mps_test_result else 'âŒ FAILED'}")
    except Exception as e:
        print(f"MPS Test: âŒ FAILED - {e}")

    # Quick model test
    print("\nðŸ¤– Testing model capabilities...")
    try:
        from modules.model_loader import get_model_capabilities
        capabilities = get_model_capabilities(model)
        for key, value in capabilities.items():
            status = "âœ…" if value else "âš ï¸" if key == 'is_causal_lm' else "â„¹ï¸"
            print(f"  {status} {key}: {value}")
    except Exception as e:
        print(f"âŒ Model capabilities test failed: {e}")

    # Vector DB test
    print("\nðŸ“š Testing vector database...")
    if vector_db:
        try:
            stats = vector_db.get_stats()
            print(f"âœ… Vector DB loaded: {stats['db_type']}")
            print(f"  Statistics: {stats}")
        except Exception as e:
            print(f"âŒ Vector DB stats failed: {e}")
    else:
        print("âš ï¸  No vector database loaded")

    # Query test
    print("\nðŸ’¬ Testing query processing...")
    test_query = "What is artificial intelligence?"
    try:
        device = get_optimal_device()
        with monitor_performance("dev_query_test"):
            result = process_query_with_context(
                test_query, model, tokenizer, device, vector_db, embedding_model
            )

        print(f"âœ… Test query result: {result['answer'][:100]}...")
        if 'score' in result:
            print(f"ðŸ“Š Score: {result['score']}")
        print(f"ðŸ–¥ï¸  Device used: {result.get('device_used', 'unknown')}")
    except Exception as e:
        print(f"âŒ Query test failed: {e}")

    print("\nðŸŽ‰ Development test completed")


def run_noise_robustness_experiment(
        vector_db: VectorDBInterface,
        embedding_model: HuggingFaceEmbedding):
    if not validate_prerequisites("noise", ACTIVE_QA_DATASET, vector_db):
        return
    db_type = vector_db.db_type
    distance_metric = vector_db.distance_metric
    timestamp = datetime.now().strftime("%d-%m-%Y_%H-%M-%S")
    default_filename = f"{db_type}_{distance_metric}_{timestamp}.xlsx"

    filename = prompt_with_validation(
        f"Enter output Excel filename (press Enter to use default: {default_filename}):\n",
        lambda s: s == "" or s.endswith('.xlsx'),
        default=default_filename
    )

    if not filename or not filename.endswith('.xlsx'):
        logger.warning("Invalid or empty filename provided. Using default name.")
        filename = default_filename

    output_dir = os.path.join(PROJECT_PATH, "results", "noise_robustness")
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, filename)

    run_noise_experiment(
        vector_db=vector_db,
        embed_model=embedding_model,
        top_k=RETRIEVER_TOP_K,
        output_path=output_path
    )


def run_retrieval_base_algorithm_experiment(
        vector_db: VectorDBInterface,
        embedding_model: HuggingFaceEmbedding,
        evaluator_model: torch.nn.Module
):
    # Validate prerequisites
    if not validate_prerequisites("retrieval_base", ACTIVE_QA_DATASET, vector_db):
        return

    print_section_header("ðŸ§ª SIMPLE RETRIEVAL BASELINE EXPERIMENT")

    # ---------------------------
    # 1. Load queries generically
    # ---------------------------
    try:
        queries = load_qa_queries(NQ_SAMPLE_SIZE)
        print(f"ðŸ“ Loaded {len(queries)} queries for experiment.")
    except Exception as e:
        print(f"âŒ Failed to load QA queries: {e}")
        logger.error(f"Failed to load queries: {e}")
        return

    # ---------------------------
    # 2. Prepare output filename
    # ---------------------------
    db_type = vector_db.db_type
    distance_metric = vector_db.distance_metric
    timestamp = datetime.now().strftime("%d-%m-%Y_%H-%M-%S")
    default_filename = f"{db_type}_{distance_metric}_top_{RETRIEVER_TOP_K}_results_retrieval_base_{timestamp}.csv"
    filename = prompt_with_validation(
        f"Enter output CSV filename (press Enter for default: {default_filename}):\n",
        lambda s: s == "" or s.endswith('.csv'),
        default=default_filename
    )

    if not filename or not filename.endswith('.csv'):
        logger.warning("Invalid filename. Using default.")
        filename = default_filename

    output_dir = os.path.join(PROJECT_PATH, "results", "retrieval_base_experiment")
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, filename)

    # ---------------------------
    # 3. Run experiment
    # ---------------------------
    run_retrieval_base_experiment(
        queries=queries,
        vector_db=vector_db,
        embed_model=embedding_model,
        top_k=RETRIEVER_TOP_K,
        output_path=output_path,
        evaluator_model=evaluator_model
    )

    print("âœ… Retrieval Baseline experiment completed.")


# --------------------------------------------------------------
# LLM Score vs Distance Scatter Experiment
# --------------------------------------------------------------

def llm_score(evaluator_model, tokenizer, query: str, document: str) -> float:
    """
    Bulletproof LLM scoring function (enhanced v2):
    - Forces output in <score>X</score>
    - Contains aggressive cleaning + multiple fallbacks
    - Handles MPS/CPU/float32 issues
    - Avoids discrete patterns (0.1/0.5/1) via soft normalization
    """

    import torch
    import re

    try:
        device = next(evaluator_model.parameters()).device

        # ---------------------
        # 1. BUILD THE PROMPT
        # ---------------------
        prompt = (
            "You are a strict numerical scoring model.\n"
            "Your ONLY task is to output a semantic relevance score between the query and the document.\n"
            "\n"
            "RULES:\n"
            "- Output MUST be in the exact XML format: <score>X</score>\n"
            "- X must be a real number between 0 and 1 with decimal precision.\n"
            "- Do NOT add explanations.\n"
            "- Do NOT add reasoning.\n"
            "- Do NOT output anything except the XML tag.\n"
            "\n"
            "Example output:\n"
            "<score>0.42</score>\n"
            "\n"
            f"Query:\n{query}\n\n"
            f"Document:\n{document}\n\n"
            "Return ONLY:\n<score>X</score>"
        )

        # ---------------------
        # 2. TOKENIZE
        # ---------------------
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # ---------------------
        # 3. GENERATE
        # ---------------------
        with torch.no_grad():
            outputs = evaluator_model.generate(
                **inputs,
                max_new_tokens=24,
                do_sample=False,
                temperature=0.0,
                num_beams=1,
                repetition_penalty=1.1
            )

        generated_ids = outputs[0][inputs["input_ids"].shape[1]:]
        text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

        # ---------------------
        # 4. STRICT XML PARSING
        # ---------------------
        xml_match = re.search(r"<score>\s*([0-9]*\.?[0-9]+)\s*</score>", text)
        if xml_match:
            val = float(xml_match.group(1))
            return float(max(0.0, min(1.0, val)))

        # ---------------------
        # 5. ANY NUMBER FALLBACK
        # ---------------------
        nums = re.findall(r"[0-9]*\.?[0-9]+", text)
        if nums:
            val = float(nums[0])
            return float(max(0.0, min(1.0, val)))

        print(f"âš ï¸ llm_score(): Could not parse model output: {text}")
        return 0.0

    except Exception as e:
        print(f"âš ï¸ llm_score exception: {e}")
        return 0.0


def crossencoder_score(cross_encoder_model, query: str, document: str) -> float:
    """
    Bulletproof scoring function for CrossEncoder models.

    - Ensures numerical output between 0 and 1
    - Handles batching issues
    - Handles CPU/MPS/CUDA automatically
    - Gracefully handles model errors
    """

    import torch
    import numpy as np

    try:
        # CrossEncoder expects a list of pairs: [(query, doc)]
        pair = [(query, document)]

        raw_score = cross_encoder_model.predict(pair)

        # ---------- Normalize output ----------
        # 1. Tensor â†’ float
        if isinstance(raw_score, torch.Tensor):
            raw_score = raw_score.detach().cpu().numpy()

        # 2. Numpy array â†’ Python float
        if isinstance(raw_score, np.ndarray):
            if raw_score.size == 1:
                raw_score = float(raw_score[0])
            else:
                raw_score = float(raw_score.mean())  # fallback safety

        # 3. Convert from logits if needed
        # Some CrossEncoders output logits in (-inf, +inf)
        # We convert them to probability using sigmoid:
        try:
            score = 1 / (1 + np.exp(-raw_score))  # sigmoid
        except Exception:
            score = float(raw_score)

        # Clamp to [0,1]
        score = float(max(0.0, min(1.0, score)))

        return score

    except Exception as e:
        print(f"âš ï¸ crossencoder_score error: {e}")
        return 0.0


def run_llm_score_vs_distance_scatter_experiment(
        vector_db: VectorDBInterface,
        tokenizer: any,
        llm_model,
        cross_encoder_model,
):
    """
    Experiment:
    For 10 random queries:
        â€¢ Sample 1000 random documents (no relation to query), ONCE.
        â€¢ Compute LLM semantic score(query, doc)
        â€¢ Compute vector distance(doc, reference_doc)
    Produce scatter plots: x = LLM score, y = embedding distance.
    """
    print_section_header("ðŸ“Š LLM SCORE vs VECTOR DISTANCE SCATTER EXPERIMENT")

    import numpy as np
    import matplotlib.pyplot as plt
    import random

    # ---------------------------------------
    # Step 0 â€” Sample documents ONCE (global)
    # ---------------------------------------
    try:
        raw_sample = vector_db.collection.peek(limit=1000)
    except Exception as e:
        print(f"âŒ Failed to peek documents from vector DB: {e}")
        return

    sample_docs = raw_sample.get("documents", []) or []
    sample_embs = raw_sample.get("embeddings")
    if sample_embs is None:
        sample_embs = []

    if len(sample_docs) == 0 or len(sample_embs) == 0:
        print("âš ï¸ No documents returned from peek(); aborting experiment.")
        return

    indices = list(range(len(sample_docs)))
    random.shuffle(indices)
    sample_docs = [sample_docs[i] for i in indices]
    sample_embs = [sample_embs[i] for i in indices]

    # ---------------------------------------
    # Step 1 â€” Load 10 queries
    # ---------------------------------------
    try:
        queries = load_qa_queries(10)
        print(f"ðŸ“ Loaded {len(queries)} queries.")
    except Exception as e:
        print(f"âŒ Failed to load QA queries: {e}")
        return

        # Ask user for filename
    from datetime import datetime
    timestamp = datetime.now().strftime("%d-%m-%Y_%H-%M-%S")
    default_base = f"llm_score_vs_vector_distance_{timestamp}"
    filename_base = prompt_with_validation(
        f"Enter base filename (without extension) or press Enter for default ({default_base}): \n",
        lambda s: True,
        default=default_base
    )

    # Step 2 â€” Prepare output directory

    # Step 3 â€” Process each query using the SAME 1000 docs
    import csv
    output_dir = os.path.join(PROJECT_PATH, "results", "scatter_experiment")
    os.makedirs(output_dir, exist_ok=True)
    for q_index, entry in enumerate(queries):
        question = entry.get("question", "")
        print(f"ðŸ” Processing query {q_index + 1}/10")

        # Initialize arrays for all possible types
        xs = []
        ys = []
        xs_llm = []
        ys_llm = []
        xs_ce = []
        ys_ce = []

        # Compute ground truth embedding for this query
        gt_id = entry.get("pubid") or entry.get("PMID") or entry.get("pmid")
        if not gt_id:
            print(f"âš ï¸ No ground truth ID found for query {q_index + 1}; skipping.")
            continue
        try:
            gt_results = vector_db.collection.get(
                where={"PMID": gt_id},
                include=["embeddings", "documents", "metadatas"]
            )
            gt_embs = gt_results.get("embeddings")
            if gt_embs is None or len(gt_embs) == 0 or gt_embs[0] is None:
                print(f"âš ï¸ No ground truth embedding found for ID {gt_id} (query {q_index + 1}); skipping.")
                continue
            gt_emb = np.array(gt_embs[0], dtype=np.float32)
        except Exception as e:
            print(f"âš ï¸ Error retrieving ground truth embedding for ID {gt_id}: {e}; skipping query.")
            continue

        # Log evaluator type
        if EVALUATOR_TYPE == "LLM":
            print("Using LLM evaluator for scoring.")
        elif EVALUATOR_TYPE == "CrossEncoder":
            print("Using CrossEncoder evaluator for scoring.")
        elif EVALUATOR_TYPE == "Both":
            print("Using BOTH LLM and CrossEncoder evaluators for scoring.")

        # Iterate through all sampled documents
        for doc_text, emb in zip(sample_docs, sample_embs):
            doc_emb = np.array(emb, dtype=np.float32)

            # ---------------------------------------
            #   SCORING SECTION â€” supports:
            #   â€¢ LLM
            #   â€¢ CrossEncoder
            #   â€¢ Both (two parallel scores)
            # ---------------------------------------

            llm_s = None
            ce_s = None

            # LLM scoring
            if EVALUATOR_TYPE in ["LLM", "Both"]:
                try:
                    llm_s = llm_score(llm_model, tokenizer, question, doc_text)
                    if isinstance(llm_s, list):
                        llm_s = llm_s[0]
                    llm_s = float(llm_s)
                except Exception as e:
                    llm_s = None

            # Crossâ€‘Encoder scoring
            if EVALUATOR_TYPE in ["CrossEncoder", "Both"]:
                try:
                    ce_s = crossencoder_score(cross_encoder_model, question, doc_text)
                    if isinstance(ce_s, list):
                        ce_s = ce_s[0]
                    ce_s = float(ce_s)
                except Exception as e:
                    ce_s = None

            # If no valid score from the selected evaluator(s), skip
            if EVALUATOR_TYPE == "LLM" and llm_s is None:
                continue
            if EVALUATOR_TYPE == "CrossEncoder" and ce_s is None:
                continue
            if EVALUATOR_TYPE == "Both" and llm_s is None and ce_s is None:
                continue

            # --- Use vector DB metric if available ---
            metric = getattr(vector_db, "distance_metric", None)
            if metric == DistanceMetric.COSINE:
                metric_func = cosine_distance
            elif metric == DistanceMetric.EUCLIDEAN:
                metric_func = lambda a, b: np.linalg.norm(a - b)
            elif metric == DistanceMetric.INNER_PRODUCT:
                metric_func = lambda a, b: -np.dot(a, b)
            else:
                metric_func = cosine_distance
            dist = metric_func(gt_emb, doc_emb)

            # Append to appropriate arrays
            if EVALUATOR_TYPE == "LLM":
                xs.append(llm_s)
                ys.append(dist)
            elif EVALUATOR_TYPE == "CrossEncoder":
                xs.append(ce_s)
                ys.append(dist)
            elif EVALUATOR_TYPE == "Both":
                # append **both** results, aligned to same distance
                if llm_s is not None:
                    xs_llm.append(llm_s)
                    ys_llm.append(dist)
                if ce_s is not None:
                    xs_ce.append(ce_s)
                    ys_ce.append(dist)

        # ---- Create new plot for this query ----
        plt.figure(figsize=(10, 6))

        if EVALUATOR_TYPE == "LLM":
            if xs and ys:
                plt.scatter(xs, ys, alpha=0.4, label="LLM Score")
            else:
                print(f"âš ï¸ No valid points for query {q_index + 1}; skipping plot.")
                plt.close()
                continue
        elif EVALUATOR_TYPE == "CrossEncoder":
            if xs and ys:
                plt.scatter(xs, ys, alpha=0.4, label="CrossEncoder Score")
            else:
                print(f"âš ï¸ No valid points for query {q_index + 1}; skipping plot.")
                plt.close()
                continue
        elif EVALUATOR_TYPE == "Both":
            has_points = False
            if xs_llm and ys_llm:
                plt.scatter(xs_llm, ys_llm, alpha=0.4, label="LLM")
                has_points = True
            if xs_ce and ys_ce:
                plt.scatter(xs_ce, ys_ce, alpha=0.4, label="CrossEncoder")
                has_points = True
            if not has_points:
                print(f"âš ï¸ No valid points for query {q_index + 1}; skipping plot.")
                plt.close()
                continue

        plt.legend()
        plt.title(f"LLM Score vs Vector Distance â€” Query {q_index + 1}")
        plt.xlabel("LLM Score (0â€“1)" if EVALUATOR_TYPE != "CrossEncoder" else "CrossEncoder Score (0â€“1)")
        plt.ylabel("Vector Distance")
        plt.grid(True)

        # Filename per query
        plot_path = os.path.join(output_dir, f"{filename_base}_query_{q_index + 1}.png")
        plt.savefig(plot_path, dpi=300)
        print(f"ðŸ“ˆ Plot saved: {plot_path}")
        plt.close()

        # ---- Save CSV for this query ----
        csv_path = os.path.join(output_dir, f"{filename_base}_query_{q_index + 1}.csv")
        with open(csv_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            if EVALUATOR_TYPE == "LLM":
                writer.writerow(["llm_score", "distance"])
                for x, y in zip(xs, ys):
                    writer.writerow([x, y])
            elif EVALUATOR_TYPE == "CrossEncoder":
                writer.writerow(["crossencoder_score", "distance"])
                for x, y in zip(xs, ys):
                    writer.writerow([x, y])
            elif EVALUATOR_TYPE == "Both":
                writer.writerow(["llm_score", "crossencoder_score", "distance"])
                max_len = max(len(xs_llm), len(xs_ce))
                for i in range(max_len):
                    row_llm = xs_llm[i] if i < len(xs_llm) else ""
                    row_ce = xs_ce[i] if i < len(xs_ce) else ""
                    row_dist = ys_llm[i] if i < len(ys_llm) else (ys_ce[i] if i < len(ys_ce) else "")
                    writer.writerow([row_llm, row_ce, row_dist])
        print(f"ðŸ“„ CSV saved: {csv_path}")

    print("âœ… Experiment completed.")


############################################################
# User-facing message and prompt helpers (centralized I/O) #
############################################################

def show_exit_message():
    """Display a goodbye message."""
    print("\nðŸ‘‹ Goodbye! Have a great day!")


def confirm_use_vector_db() -> bool:
    """Ask user if they want to use a vector database."""
    return ask_yes_no("Do you want to use vector database for context retrieval? (y/n): ", default='y')


def show_vector_db_success():
    """Display a success message after loading the vector database."""
    print("âœ… Vector DB loaded successfully")


def show_error_message(message: str):
    """Display a formatted error message."""
    print(f"âŒ {message}")


def show_goodbye_message():
    """Alias for exit message."""
    show_exit_message()


def confirm_reset_vector_db() -> bool:
    """Ask user to confirm vector DB reset."""
    return ask_yes_no("Are you sure you want to reset the Vector DB and Embedding Model? (y/n): ", default='n')


def show_mode_choice_banner():
    """Display a banner for selecting execution mode."""
    print("\nðŸ“Š SELECT EXECUTION MODE")
    print("-" * 40)
    print("1. Enumeration")
    print("2. Hill Climbing")
    print("3. Trilateration")
    print("-" * 40)


def show_evaluation_completed():
    """Display completion message for evaluation."""
    print("âœ… Evaluation completed successfully.")


def show_performance_summary_notice():
    """Display performance summary location notice."""
    print("\nðŸ§¾ Performance summary saved to: results/performance_metrics.json")
    print("ðŸ“Š You can view it later or print it now.")
